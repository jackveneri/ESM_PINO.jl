var documenterSearchIndex = {"docs":
[{"location":"extensions/QG3/#QG3-Extension","page":"QG3 Extension","title":"QG3 Extension","text":"","category":"section"},{"location":"extensions/QG3/","page":"QG3 Extension","title":"QG3 Extension","text":"This page documents the QG3-based SFNO layers and utility functions defined to work with the QG3 package","category":"page"},{"location":"extensions/QG3/#ESM_PINO.SFNO-Tuple{QG3.GaussianGridtoSHTransform, QG3.SHtoGaussianGridTransform}","page":"QG3 Extension","title":"ESM_PINO.SFNO","text":"SFNO(\n    ggsh::QG3.GaussianGridtoSHTransform,\n    shgg::QG3.SHtoGaussianGridTransform;\n    modes,\n    in_channels,\n    out_channels,\n    hidden_channels,\n    n_layers,\n    lifting_channel_ratio,\n    projection_channel_ratio,\n    channel_mlp_expansion,\n    activation,\n    positional_embedding,\n    inner_skip,\n    outer_skip,\n    zsk,\n    use_norm,\n    downsampling_factor,\n    gpu,\n    batch_size\n) -> Union{SFNO{GridEmbedding2D, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A))}, SFNO{Lux.NoOpLayer, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A))}}\n\n\nSpherical Fourier Neural Operator (SFNO) layer combining positional embeddings, spectral kernels, and channel MLPs.\n\nThis layer implements the SFNO architecture on the sphere, optionally using Zonal Symmetric Kernels (ZSK) following the approach described in Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere.\n\nArguments\n\nggsh::QG3.GaussianGridtoSHTransform: Precomputed grid-to-SH transform.\nshgg::QG3.SHtoGaussianGridTransform: Precomputed SH-to-grid transform.\nOther keyword arguments are the same as for the primary constructor, except modes which default is set to ggsh.output_size[1]. Also, no need to specify batch_size or gpu as these are handled in the transforms.\n\nReturns\n\nSFNO: A Lux-compatible container layer.\n\nDetails\n\nConstructs lifting, SFNO blocks, and projection layers compatible with Lux.jl.\nPositional embeddings are appended if positional_embedding=\"grid\".\nSupports both CPU and GPU execution.\nZonal Symmetric Kernels (ZSK) reduce the number of parameters and improve stability on spherical domains.\n\nExample\n\nusing Lux, QG3, Random, NNlib, LuxCUDA\n\n# Load precomputed QG3 parameters\nqg3ppars = QG3.load_precomputed_params()[2]\n\n# Input: [lat, lon, channels, batch]\nx = rand(Float32, 32, 64, 3, 10)\n\n\n# Construct SFNO layer using secondary constructor\nggsh = QG3.GaussianGridtoSHTransform(qg3ppars, 32, N_batch=size(x,4))\nshgg = QG3.SHtoGaussianGridTransform(qg3ppars, 32, N_batch=size(x,4))\nmodel2 = SFNO(ggsh, shgg;\n    modes=15,\n    in_channels=3,\n    out_channels=3,\n    hidden_channels=32,\n    n_layers=4,\n    lifting_channel_ratio=2,\n    projection_channel_ratio=2,\n    channel_mlp_expansion=2.0,\n    positional_embedding=\"no_grid\",\n    outer_skip=true,\n    zsk=true\n)\n\n# Setup parameters and state\nrng = Random.default_rng(0)\nps, st = Lux.setup(rng, model2)\n\n# Forward pass\ny, st = model2(x, ps, st)\n\n# Compute gradients\nusing Zygote\ngr = Zygote.gradient(ps -> sum(model2(x, ps, st)[1]), ps)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINO.SFNO-Tuple{QG3ModelParameters}","page":"QG3 Extension","title":"ESM_PINO.SFNO","text":"SFNO(\n    pars::QG3ModelParameters;\n    batch_size,\n    modes,\n    in_channels,\n    out_channels,\n    hidden_channels,\n    n_layers,\n    lifting_channel_ratio,\n    projection_channel_ratio,\n    channel_mlp_expansion,\n    activation,\n    positional_embedding,\n    inner_skip,\n    outer_skip,\n    zsk,\n    use_norm,\n    downsampling_factor,\n    gpu\n) -> Union{SFNO{GridEmbedding2D, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A))}, SFNO{Lux.NoOpLayer, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A))}}\n\n\nSpherical Fourier Neural Operator (SFNO) layer combining positional embeddings, spectral kernels, and channel MLPs.\n\nThis layer implements the SFNO architecture on the sphere, optionally using Zonal Symmetric Kernels (ZSK) following the approach described in Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere.\n\nArguments\n\npars::QG3ModelParameters: Model parameters defining the spherical grid and maximum spherical harmonic degree L.\nbatch_size::Int=1: Number of samples in a batch.\nmodes::Int=pars.L: Maximum number of spherical harmonic modes to use.\nin_channels::Int: Number of input channels.\nout_channels::Int: Number of output channels.\nhidden_channels::Int=32: Number of hidden channels in the SFNO blocks.\nn_layers::Int=4: Number of SFNO blocks.\nlifting_channel_ratio::Int=2: Expansion ratio for the lifting layer.\nprojection_channel_ratio::Int=2: Expansion ratio for the projection layer.\nchannel_mlp_expansion::Number=2.0: Expansion factor for channel MLPs in each SFNO block.\nactivation: Activation function (default is NNlib.gelu).\npositional_embedding::AbstractString=\"grid\": Type of positional embedding. Options: \"grid\" or \"no_grid\".\ninner_skip::Bool=true: If true, use skip connections inside each SFNO block.\nouter_skip::Bool=true: If true, apply residual connection from lifting output to projection output.\ngpu::Bool=true: If true, computations are performed on GPU.\nzsk::Bool=false: If true, use Zonal Symmetric Kernels, enforcing longitudinal symmetry.\n\nReturns\n\nSFNO: A Lux-compatible container layer.\n\nDetails\n\nConstructs lifting, SFNO blocks, and projection layers compatible with Lux.jl.\nPositional embeddings are appended if positional_embedding=\"grid\".\nSupports both CPU and GPU execution.\nZonal Symmetric Kernels (ZSK) reduce the number of parameters and improve stability on spherical domains.\n\nExample\n\nusing Lux, QG3, Random, NNlib, LuxCUDA\n\n# Load precomputed QG3 parameters\nqg3ppars = QG3.load_precomputed_params()[2]\n\n# Input: [lat, lon, channels, batch]\nx = rand(Float32, 32, 64, 3, 10)\n\n# Construct SFNO layer using primary constructor\nmodel1 = SFNO(qg3ppars;\n    batch_size=size(x, 4),\n    modes=30,\n    in_channels=3,\n    out_channels=3,\n    hidden_channels=32,\n    n_layers=4,\n    lifting_channel_ratio=2,\n    projection_channel_ratio=2,\n    channel_mlp_expansion=2.0,\n    positional_embedding=\"no_grid\",\n    outer_skip=true,\n    gpu=false\n)\n\n\n# Setup parameters and state\nrng = Random.default_rng(0)\nps, st = Lux.setup(rng, model1)\n\n# Forward pass\ny, st = model1(x, ps, st)\n\n# Compute gradients\nusing Zygote\ngr = Zygote.gradient(ps -> sum(model1(x, ps, st)[1]), ps)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINO.SFNO_Block-Tuple{Int64, QG3.GaussianGridtoSHTransform, QG3.SHtoGaussianGridTransform}","page":"QG3 Extension","title":"ESM_PINO.SFNO_Block","text":"SFNO_Block(\n    channels::Int64,\n    ggsh::QG3.GaussianGridtoSHTransform,\n    shgg::QG3.SHtoGaussianGridTransform;\n    modes,\n    expansion_factor,\n    activation,\n    skip,\n    zsk,\n    use_norm\n) -> ESM_PINO.SFNO_Block{ESM_PINOQG3Ext.ESM_PINOQG3}\n\n\nA block that combines a spherical kernel with a channel MLP. Expects input in (spatial..., channel, batch) format.\n\nArguments\n\nchannels::Int: Number of input/output channels\nggsh::GaussianGridtoSHTransform: Transformation from Gaussian grid to spherical harmonics\nshgg::SHtoGaussianGridTransform: Transformation from spherical harmonics back to Gaussian\nmodes::Int=ggsh.output_size[1]: Number of spherical harmonic modes to retain (default: ggsh.output_size[1])\nexpansion_factor::Real=2.0: Expansion factor for the ChannelMLP (default: 2.0)\nactivation: Activation function applied after combining spatial and spectral branches (default: NNlib.gelu)\nskip::Bool=true: Whether to include a skip connection (default: true)\nzsk::Bool=false: Whether to use Zonal Symmetric Kernels (ZSK) (default: false)\n\nReturns\n\nSFNO_Block: A Lux-compatible layer operating on 4D arrays [lat, lon, channels, batch].\n\nFields\n\nspherical_kernel::SphericalKernel: Spherical kernel layer\nchannel_mlp::ChannelMLP: Channel-wise MLP layer\nchannels::Int: Number of input/output channels\nskip::Bool: Whether to include a skip connection\n\nDetails\n\nThe input is processed by a SphericalKernel followed by a ChannelMLP\nIf skip is true, the input is added to the output (residual connection)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINO.SFNO_Block-Tuple{Int64, QG3ModelParameters}","page":"QG3 Extension","title":"ESM_PINO.SFNO_Block","text":"SFNO_Block(\n    channels::Int64,\n    pars::QG3ModelParameters;\n    modes,\n    batch_size,\n    expansion_factor,\n    activation,\n    skip,\n    gpu,\n    zsk,\n    use_norm\n) -> ESM_PINO.SFNO_Block{ESM_PINOQG3Ext.ESM_PINOQG3}\n\n\nA block that combines a spherical kernel with a channel MLP. Expects input in (spatial..., channel, batch) format.\n\nArguments\n\nchannels::Int: Number of input/output channels\npars::QG3ModelParameters: Precomputed QG3 model parameters (QG3ModelParameters)\nmodes::Int=pars.L: Number of spherical harmonic modes to retain (default: pars.L)\nbatch_size::Int=1: Batch size for transforms (default: 1)\nexpansion_factor::Real=2.0: Expansion factor for the ChannelMLP (default: 2.0)\nactivation: Activation function applied after combining spatial and spectral branches (default: NNlib.gelu)\nskip::Bool=true: Whether to include a skip connection (default: true)\ngpu::Bool=true: Whether to use GPU (default: true)\nzsk::Bool=false: Whether to use Zonal Symmetric Kernels (ZSK) (default: false)\n\nReturns\n\nSFNO_Block: A Lux-compatible layer operating on 4D arrays [lat, lon, channels, batch].\n\nFields\n\nspherical_kernel::SphericalKernel: Spherical kernel layer\nchannel_mlp::ChannelMLP: Channel-wise MLP layer\nchannels::Int: Number of input/output channels\nskip::Bool: Whether to include a skip connection\n\nDetails\n\nThe input is processed by a SphericalKernel followed by a ChannelMLP\nIf skip is true, the input is added to the output (residual connection)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINO.SphericalConv","page":"QG3 Extension","title":"ESM_PINO.SphericalConv","text":"SphericalConv(\n    hidden_channels::Int64,\n    ggsh::QG3.GaussianGridtoSHTransform,\n    shgg::QG3.SHtoGaussianGridTransform;\n    ...\n) -> ESM_PINO.SphericalConv{ESM_PINOQG3Ext.ESM_PINOQG3}\nSphericalConv(\n    hidden_channels::Int64,\n    ggsh::QG3.GaussianGridtoSHTransform,\n    shgg::QG3.SHtoGaussianGridTransform,\n    modes::Int64;\n    zsk\n) -> ESM_PINO.SphericalConv{ESM_PINOQG3Ext.ESM_PINOQG3}\n\n\nSpherical convolution layer for functions on the sphere using spherical harmonics.   Transforms data from Gaussian grid → spherical harmonics, applies learned weights, and transforms back.\n\nArguments\n\nhidden_channels::Int: Number of input/output channels.\nggsh::GaussianGridtoSHTransform: Transformation from Gaussian grid to spherical harmonics.\nshgg::SHtoGaussianGridTransform: Transformation from spherical harmonics back to Gaussian grid.\nmodes::Int=ggsh.output_size[1]: Maximum number of spherical harmonic modes to use. If higher than ggsh.output_size[1], it is truncated with a warning.\nzsk::Bool=false: If true, uses Zonal Symmetric Kernels (ZSK), reducing the number of free weights. It follows Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere.\n\nReturns\n\nSphericalConv: A Lux-compatible layer operating on 4D arrays [lat, lon, channels, batch].\n\nDetails\n\nInput is permuted internally to [channels, lat, lon, batch] for computation.\nUses ps.weight for element-wise multiplication in spherical harmonic space.\nSupports padding to match the original spherical grid dimensions.\nCompatible with GPU and CPU (controlled externally in ggsh, shgg constructor).\n\nExample\n\nusing Random, Lux, QG3, NNlib\n\n# Load precomputed spherical model parameters\nqg3ppars = QG3.load_precomputed_params()[2]\n\n# Create transforms\nggsh = QG3.GaussianGridtoSHTransform(qg3ppars, 32, N_batch=1)\nshgg = QG3.SHtoGaussianGridTransform(qg3ppars, 32, N_batch=1)\n\n# Initialize layer\nlayer = SphericalConv(32, ggsh, shgg, 30; zsk=true)\n\n# Generate random input [lat, lon, channels, batch]\nx = rand(Float32, 32, 64, 32, 1)\n\n# Setup parameters and state\nrng = Random.default_rng(0)\nps, st = Lux.setup(rng, layer)\n\n# Forward pass\ny, st = layer(x, ps, st)\n\n# Compute gradient\nusing Zygote\ngr = Zygote.gradient(ps -> sum(layer(x, ps, st)[1]), ps)\n\n\n\n\n\n","category":"type"},{"location":"extensions/QG3/#ESM_PINO.SphericalConv-Union{Tuple{T}, Tuple{QG3ModelParameters{T, I, A, M} where {I<:Int64, A<:AbstractVector{T}, M<:AbstractMatrix{T}}, Int64}} where T","page":"QG3 Extension","title":"ESM_PINO.SphericalConv","text":"SphericalConv(\n    pars::QG3ModelParameters{T, I, A, M} where {I<:Int64, A<:AbstractArray{T, 1}, M<:AbstractArray{T, 2}},\n    hidden_channels::Int64;\n    modes,\n    batch_size,\n    gpu,\n    zsk\n) -> ESM_PINO.SphericalConv{ESM_PINOQG3Ext.ESM_PINOQG3}\n\n\nConstruct a spherical convolution layer using precomputed model parameters.\n\nArguments\n\npars::QG3.QG3ModelParameters{T}: Model parameters defining the spherical grid resolution and maximum spherical harmonic degree L.\nhidden_channels::Int: Number of input/output channels.\nmodes::Int=pars.L: Maximum number of spherical harmonic modes to use. If higher than pars.L, it is truncated with a warning.\nbatch_size::Int=1: Number of samples in a batch (used for internal transforms).\ngpu::Bool=true: If true, computations are moved to GPU using QG3.gpuon().\nzsk::Bool=false: If true, uses Zonal Symmetric Kernels (ZSK), reducing the number of free weights. ZSK enforces rotational symmetry along longitude and follows Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere.\n\nReturns\n\nSphericalConv: A Lux-compatible layer operating on 4D arrays [lat, lon, channels, batch].\n\nDetails\n\nInternally constructs GaussianGridtoSHTransform and SHtoGaussianGridTransform objects for the given pars and hidden_channels.\nCorrects the requested modes to not exceed pars.L.\nSupports both CPU and GPU computation.\nZonal Symmetric Kernels (ZSK) reduce the number of learnable parameters by enforcing symmetry along longitude, improving stability for spherical dynamics.\n\nExample\n\nusing Random, Lux, QG3, NNlib\n\n# Load precomputed parameters\nqg3ppars = QG3.load_precomputed_params()[2]\n\n# Initialize spherical convolution layer\nlayer = SphericalConv(qg3ppars, 32; modes=30, batch_size=1, gpu=false, zsk=true)\n\n# Generate random input [lat, lon, channels, batch]\nx = rand(Float32, 64, 128, 32, 1)\n\n# Setup parameters and state\nrng = Random.default_rng(0)\nps, st = Lux.setup(rng, layer)\n\n# Forward pass\ny, st = layer(x, ps, st)\n\n# Compute gradient\nusing Zygote\ngr = Zygote.gradient(ps -> sum(layer(x, ps, st)[1]), ps)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINO.SphericalKernel","page":"QG3 Extension","title":"ESM_PINO.SphericalKernel","text":"SphericalKernel(\n    hidden_channels::Int64,\n    pars::QG3ModelParameters;\n    ...\n) -> ESM_PINO.SphericalKernel{ESM_PINOQG3Ext.ESM_PINOQG3}\nSphericalKernel(\n    hidden_channels::Int64,\n    pars::QG3ModelParameters,\n    activation;\n    modes,\n    batch_size,\n    gpu,\n    zsk\n) -> ESM_PINO.SphericalKernel{ESM_PINOQG3Ext.ESM_PINOQG3}\n\n\nCombines a SphericalConv layer with a 1x1 convolution in parallel, followed by an activation function. Expects input in (spatial..., channel, batch) format.\n\nArguments\n\nhidden_channels: Number of channels\npars: Precomputed QG3 model parameters (QG3ModelParameters)\nactivation: Activation function applied after combining spatial and spectral branches (default: NNlib.gelu)\nmodes: Number of spherical harmonic modes to retain (default: pars.L)\nbatch_size: Batch size for transforms (default: 1)\ngpu: Whether to use GPU (default: true)\nzsk: Whether to use Zonal Symmetric Kernels (ZSK) (default: false)\n\n#Returns\n\nSphericalKernel: A Lux-compatible layer operating on 4D arrays `[lat,\n\nFields\n\nspatial_conv::P: 1x1 convolution operating directly in the spatial domain\nspherical_conv::SphericalalConv: Spherical convolution layer\nactivation::F: Elementwise activation function\n\nDetails\n\nThe input is processed in parallel by a 1x1 convolution and a spherical convolution\nOutputs from both branches are summed and passed through the activation\nUseful for mixing local (spatial) and global (spectral) information\n\n\n\n\n\n","category":"type"},{"location":"extensions/QG3/#ESM_PINO.SphericalKernel-2","page":"QG3 Extension","title":"ESM_PINO.SphericalKernel","text":"SphericalKernel(\n    hidden_channels::Int64,\n    ggsh::QG3.GaussianGridtoSHTransform,\n    shgg::QG3.SHtoGaussianGridTransform;\n    ...\n) -> ESM_PINO.SphericalKernel{ESM_PINOQG3Ext.ESM_PINOQG3}\nSphericalKernel(\n    hidden_channels::Int64,\n    ggsh::QG3.GaussianGridtoSHTransform,\n    shgg::QG3.SHtoGaussianGridTransform,\n    activation;\n    modes,\n    zsk\n) -> ESM_PINO.SphericalKernel{ESM_PINOQG3Ext.ESM_PINOQG3}\n\n\nConstruct a SphericalKernel layer using precomputed transforms.\n\nArguments\n\nhidden_channels::Int: Number of channels\nggsh::GaussianGridtoSHTransform: Transformation from Gaussian grid to spherical harmonics\nshgg::SHtoGaussianGridTransform: Transformation from spherical harmonics back to Gaussian grid\nactivation: Activation function applied after combining spatial and spectral branches (default: NNlib.gelu)\nmodes::Int=ggsh.output_size[1]: Number of spherical harmonic modes to retain (default: ggsh.output_size[1])\nzsk::Bool=false: Whether to use Zonal Symmetric Kernels (ZSK) (default: false)\n\nReturns\n\nSphericalKernel: A Lux-compatible layer operating on 4D arrays [lat, lon, channels, batch].\n\nFields\n\nspatial_conv::P: 1x1 convolution operating directly in the spatial domain\nspherical_conv::SphericalalConv: Spherical convolution layer\nactivation::F: Elementwise activation function\n\n\n\n\n\n","category":"type"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.QG3_Physics_Parameters-Tuple{}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.QG3_Physics_Parameters","text":"QG3_Physics_Parameters(\n;\n    n_lat,\n    modes,\n    batch_size\n) -> ESM_PINOQG3Ext.QG3_Physics_Parameters\n\n\nHelper constructor to pass as empty default to train_model\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.fine_tuning-Tuple{AbstractArray, AbstractArray, Any, NamedTuple, NamedTuple}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.fine_tuning","text":"fine_tuning(\n    x::AbstractArray,\n    target::AbstractArray,\n    model,\n    ps::NamedTuple,\n    st::NamedTuple;\n    n_steps,\n    maxiters,\n    lr_0,\n    parameters,\n    use_physics,\n    geometric,\n    α\n) -> LuxCore.StatefulLuxLayerImpl.StatefulLuxLayer{Val{true}, var\"#s179\", _A, NamedTuple{names, T}} where {var\"#s179\"<:LuxCore.AbstractLuxLayer, _A, names, T<:Tuple}\n\n\nFine-tune a pretrained SFNO model using an autoregressive (AR) loss function.   This procedure is typically applied after initial training to improve multi-step forecast accuracy.\n\nThe function performs a short fine-tuning loop with autoregressive supervision, optionally including a physics-informed loss component.\n\nArguments\n\nx::AbstractArray: Input data tensor.\ntarget::AbstractArray: Target data tensor with shape (lat, lon, channels, batch, time).\nmodel: Pretrained SFNO model to be fine-tuned.\nps::NamedTuple: Model parameters (from previous training).\nst::NamedTuple: Model internal state.\n\nKeywords\n\nn_steps::Int=2: Number of autoregressive steps in the loss function.\nmaxiters::Int=5: Maximum number of fine-tuning iterations.\nlr_0::Float64=1e-5: Learning rate for fine-tuning.\nparameters::QG3_Physics_Parameters=QG3_Physics_Parameters(): Physical parameters used in the loss.\nuse_physics::Bool=true: Include physics-informed component in the loss if true.\ngeometric::Bool=true: Use geometric formulation of the physics loss.\nα::Float32=0.7f0: Weighting factor between physics and data loss terms.\n\nReturns\n\nStatefulLuxLayer{true}: Fine-tuned model instance with updated parameters and states.\n\nNotes\n\nThe target tensor must have five dimensions, with the number of autoregressive steps as the fifth dimension.\nThe time dimension (size(target, 5)) must match the number of autoregressive steps (n_steps).\n\nExample\n\n# Fine-tune a pretrained SFNO model for multi-step forecasting\nft_model = fine_tuning(x_val, y_val, pretrained_model, ps, st;\n                       n_steps=3, maxiters=10, lr_0=1e-5)\n\n# Evaluate fine-tuned model\npred = ft_model(x_val)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.make_QG3_loss-Tuple{ESM_PINOQG3Ext.QG3_Physics_Parameters}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.make_QG3_loss","text":"make_QG3_loss(pars::QG3_Physics_Parameters;\n              α=0.5f0,\n              use_physics::Bool=true,\n              geometric::Bool=false)\n\nCreate a composite QG3 loss function suitable for Lux training. Returns a callable (model, ps, st, (input, target)) -> (loss, st, metrics).\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.make_autoregressive_loss-Tuple{Function}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.make_autoregressive_loss","text":"make_autoregressive_loss(QG3_loss::Function; steps::Int, sequential::Bool=true)\n\nCreate an autoregressive loss function that rolls out predictions over steps and accumulates the loss defined by QG3_loss.\n\nArguments\n\nQG3_loss: A loss function of the form (model, ps, st, (input, target)) -> (loss, st, details)\nsteps: Number of autoregressive rollout steps\nsequential: If true, predictions are fed sequentially (standard autoregressive); if false, all predictions are computed and compared in batch for efficiency.\n\nReturns\n\nA loss function (model, ps, st, (u_t1, targets)) -> (loss, st, details)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.remap_plan","page":"QG3 Extension","title":"ESM_PINOQG3Ext.remap_plan","text":"remap_plan(\n    l::Integer,\n    c::Integer;\n    ...\n) -> NamedTuple{(:remap_fixed, :mask, :new_indices), <:Tuple{Any, Any, Any}}\nremap_plan(\n    l::Integer,\n    c::Integer,\n    T::Type{<:Number};\n    gpu\n) -> NamedTuple{(:remap_fixed, :mask, :new_indices), <:Tuple{Any, Any, Any}}\n\n\nPrecompute a symmetric remapping plan between index ranges [-l..l] and [-c..c+1). This is used to ensure correct ordering and slicing of spectral coefficients on gpu\n\nArguments\n\n-l::Integer: Original spectral truncation (maximum mode index).\n\n-c::Integer: half-length of gpu-array rows.\n\n-T::Type{<:Number}=Float32: Element type for masks and indices.\n\nKeywords\n\ngpu::Bool=true: Whether to allocate the mask on the GPU (CuArray).\n\nReturns\n\nNamedTuple with:\n\nremap_fixed::Vector{Int} — Mapped indices (0 replaced with 1).\nmask::AbstractArray — Binary mask (CPU or GPU) for valid indices.\nnew_indices::Vector{Int} — Target symmetric indices.\n\nExample\n\nplan = remap_plan(42, 63; gpu=false)\n\n\n\n\n\n","category":"function"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.remap_symmetric_dim-Tuple{AbstractArray, Any}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.remap_symmetric_dim","text":"remap_symmetric_dim(\n    A::AbstractArray,\n    plan;\n    dim,\n    fill_value\n) -> Tuple{Any, Any}\n\n\nApply a precomputed symmetric remapping plan along a given array dimension. Works on both CPU and GPU arrays and is compatible with Zygote for AD.\n\nArguments\n\n-A::AbstractArray: Input tensor to be remapped.\n\n-plan: Remapping structure returned by remap_plan.\n\nKeywords\n\n-dim::Integer=3: Dimension along which to apply the remap.\n\n-fill_value: Optional value for indices outside the valid range. Defaults to zero(eltype(A)).\n\nReturns\n\n(C, new_indices): Tuple with the remapped array and the list of new indices.\n\nExample\n\nplan = remap_plan(42, 63)\nA_new, idx = remap_symmetric_dim(A, plan; dim=3)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.stack_time_steps-Union{Tuple{T}, Tuple{AbstractArray{T, 4}, Int64}} where T","page":"QG3 Extension","title":"ESM_PINOQG3Ext.stack_time_steps","text":"stack_time_steps(\n    data::AbstractArray{T, 4},\n    time_steps::Int64\n) -> Any\n\n\nConvert a 4D tensor of sequential data (lat, lon, channels, batch) into a 5D tensor suitable for autoregressive training or evaluation.   The function constructs overlapping sequences along the batch dimension, each containing time_steps consecutive snapshots.\n\nArguments\n\ndata::AbstractArray{T,4}: Input data tensor with dimensions (lat, lon, channels, batch).\ntime_steps::Int: Number of consecutive time steps to include in each sequence.\n\nReturns\n\nAbstractArray{T,5}: A 5D tensor of shape (lat, lon, channels, time_steps, n_sequences), where n_sequences = batch - time_steps + 1.\n\nNotes\n\nThe resulting array can be used as autoregressive training targets for multi-step prediction.\nSequences are created by sliding a window of length time_steps along the batch axis.\n\nExample\n\n# Input: 4D array with 10 time samples\ndata = rand(Float32, 64, 128, 3, 10)\n\n# Stack into 5D sequences of 4 time steps each\nseq_data = stack_time_steps(data, 4)\n\n@assert size(seq_data) == (64, 128, 3, 4, 7)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.train_model-Tuple{AbstractArray, AbstractArray, QG3ModelParameters}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.train_model","text":"train_model(\n    x::AbstractArray,\n    target::AbstractArray,\n    pars::QG3ModelParameters;\n    seed,\n    maxiters,\n    batchsize,\n    modes,\n    in_channels,\n    out_channels,\n    hidden_channels,\n    n_layers,\n    lifting_channel_ratio,\n    projection_channel_ratio,\n    channel_mlp_expansion,\n    activation,\n    positional_embedding,\n    inner_skip,\n    outer_skip,\n    zsk,\n    use_norm,\n    downsampling_factor,\n    lr_0,\n    parameters,\n    use_physics,\n    geometric,\n    α\n) -> Union{LuxCore.StatefulLuxLayerImpl.StatefulLuxLayer{Val{true}, SFNO{GridEmbedding2D, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3}, _A1, NamedTuple{names, T}} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A1, names, T<:Tuple}, LuxCore.StatefulLuxLayerImpl.StatefulLuxLayer{Val{true}, SFNO{Lux.NoOpLayer, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3}, _A1, NamedTuple{names, T}} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A1, names, T<:Tuple}}\n\n\nTrain an SFNO model with the possibility using a combined data-driven (simple MSE or geometrically weighted) and physics-informed loss. This function initializes the model, optimizer, and training loop, and performs iterative optimization of the model parameters.  \n\nBoth standard data loss and an optional physics-informed term are tracked during training.\n\nArguments\n\nx::AbstractArray: Input training data tensor.\ntarget::AbstractArray: Target (ground truth) data tensor.\npars::QG3ModelParameters: Model configuration including grid and spectral parameters.\n\nKeywords\n\nseed::Int=0: Random seed for reproducibility.\nmaxiters::Int=20: Number of training iterations.\nbatchsize::Int=256: Mini-batch size for training.\nmodes::Int=pars.L: Spectral truncation level.\nin_channels::Int=3: Number of input channels.\nout_channels::Int=3: Number of output channels.\nhidden_channels::Int=256: Width of the hidden feature layers.\nn_layers::Int=4: Number of SFNO layers.\nlifting_channel_ratio::Int=2: Ratio of lifting layer expansion.\nprojection_channel_ratio::Int=2: Ratio of projection layer contraction.\nchannel_mlp_expansion::Number=2.0: Expansion factor in channel MLP blocks.\nactivation: Activation function used in SFNO blocks (default: NNlib.gelu).\npositional_embedding::AbstractString=\"grid\": Type of positional embedding (\"grid\" or \"no_grid\").\ninner_skip::Bool=true: Whether to enable residual connections inside SFNO blocks.\nouter_skip::Bool=true: Whether to enable skip connections between lifting output and projection input.\nzsk::Bool=false: Use zonally symmetric kernel formulation if true.\nuse_norm::Bool=false: Apply normalization layers inside SFNO blocks.\ndownsampling_factor::Int=2: Ratio of downsampling between layers.\nlr_0::Float64=1e-3: Initial learning rate for the optimizer.\nparameters::QG3_Physics_Parameters=QG3_Physics_Parameters(pars, batch_size=batchsize): Physical parameters used in the QG3 loss.\nuse_physics::Bool=true: Whether to include the physics-informed loss component.\ngeometric::Bool=true: Use geometrically weighted formulation for the data loss.\nα::Float32=0.7f0: Weighting factor between physics loss and data loss.\n\nReturns\n\nStatefulLuxLayer{true}: Trained SFNO model containing learned parameters and internal state.\n\nExample\n\n# Initialize parameters and data\npars = qg3pars_constructor_helper(42, 64)\nx, y = generate_training_data(pars)\n\n# Train SFNO model\ntrained_model = train_model(x, y, pars; maxiters=100, batchsize=128, lr_0=5e-4)\n\n# Perform inference\npred = trained_model(x)\n\n\n\n\n\n","category":"method"},{"location":"extensions/QG3/#ESM_PINOQG3Ext.transfer_SFNO_model-Tuple{SFNO, QG3ModelParameters}","page":"QG3 Extension","title":"ESM_PINOQG3Ext.transfer_SFNO_model","text":"transfer_SFNO_model(\n    model::SFNO,\n    qg3ppars::QG3ModelParameters;\n    batch_size\n) -> Union{SFNO{GridEmbedding2D, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A))}, SFNO{Lux.NoOpLayer, L, _A, P, ESM_PINOQG3Ext.ESM_PINOQG3} where {L<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A)), _A, P<:(Lux.Chain{__T_layers, Nothing} where __T_layers<:(NamedTuple{_A, <:Tuple{Vararg{LuxCore.AbstractLuxLayer}}} where _A))}}\n\n\nConstruct a new SFNO model that replicates the architecture and parameters of an existing model, but adapts them to a new discretization (qg3ppars) and batch size.   This function preserves all spectral modes, channels, and hyperparameters while adjusting the internal transform plans to match the new grid configuration.\n\nArguments\n\nmodel::SFNO: Source SFNO model whose architecture and parameters will be cloned.\nqg3ppars: Target problem parameters (e.g., grid, spectral resolution).\n\nKeywords\n\nbatch_size::Int: Optional new batch size. Defaults to the batch size inferred from model.sfno_blocks.layers.layer_1.spherical_kernel.spherical_conv.plan.ggsh.FT_4d.plan.input_size[4].\n\nReturns\n\nSFNO: A new model instance with the same architecture as model, configured for the target discretization and batch size.\n\nExample\n\n# Original model (batch size = 32)\nmodel = SFNO(orig_pars; batch_size=32, ...)\n\n# Transfer model to a finer grid and larger batch\nnew_model = transfer_SFNO_model(model, new_pars; batch_size=64)\n\n# Forward pass with transferred weights\nŷ = new_model(x, ps, st)\n\n\n\n\n\n","category":"method"},{"location":"extensions/SpeedyWeather/#Speedy-Weather-Extension","page":"SpeedyWeather Extension","title":"Speedy Weather Extension","text":"","category":"section"},{"location":"extensions/SpeedyWeather/","page":"SpeedyWeather Extension","title":"SpeedyWeather Extension","text":"This page documents the SpeedyWeather-based SFNO layers","category":"page"},{"location":"extensions/SpeedyWeather/#ESM_PINOSpeedyWeatherExt.GaussianGridInfo","page":"SpeedyWeather Extension","title":"ESM_PINOSpeedyWeatherExt.GaussianGridInfo","text":"GaussianGridInfo\n\nStructure containing information about a Gaussian grid resolution.\n\nFields\n\ntruncation::Int: Spectral truncation number (e.g., 31 for T31)\nnlat::Int: Number of latitude points\nnlon::Int: Number of longitude points  \nkm_at_equator::Float64: Approximate grid spacing at equator in km\ndeg_at_equator::Float64: Approximate grid spacing at equator in degrees\ndescription::String: Human-readable description\n\n\n\n\n\n","category":"type"},{"location":"extensions/SpeedyWeather/#ESM_PINOSpeedyWeatherExt.calculate_gaussian_grid_size-Tuple{Int64}","page":"SpeedyWeather Extension","title":"ESM_PINOSpeedyWeatherExt.calculate_gaussian_grid_size","text":"calculate_gaussian_grid_size(truncation::Int) -> Tuple{Int, Int}\n\nCalculate Gaussian grid dimensions from spectral truncation number using standard formulas.\n\nFor a spectral truncation T, the standard relationships are:\n\nnlat = (truncation + 1) * 3 / 2  (for reduced grids, varies slightly)\nnlon = 2 * nlat  (for regular grids)\n\nArguments\n\ntruncation::Int: Spectral truncation number\n\nReturns\n\nTuple{Int, Int}: (nlat, nlon)\n\n\n\n\n\n","category":"method"},{"location":"extensions/SpeedyWeather/#ESM_PINOSpeedyWeatherExt.gaussian_resolution_to_grid-Tuple{AbstractString}","page":"SpeedyWeather Extension","title":"ESM_PINOSpeedyWeatherExt.gaussian_resolution_to_grid","text":"gaussian_resolution_to_grid(resolution::AbstractString) -> Tuple{Int, Int}\n\nConvert a Gaussian grid resolution string (e.g., \"T31\", \"T63\") to (nlat, nlon) tuple.\n\nArguments\n\nresolution::AbstractString: Grid resolution in format \"TN\" where N is truncation number\n\nReturns\n\nTuple{Int, Int}: (number of latitude points, number of longitude points)\n\nExamples\n\njulia> gaussian_resolution_to_grid(\"T31\")\n(48, 96)\n\njulia> gaussian_resolution_to_grid(\"T63\")  \n(96, 192)\n\njulia> gaussian_resolution_to_grid(\"T255\")\n(256, 512)\n\nThrows\n\nArgumentError: If resolution is not recognized\n\n\n\n\n\n","category":"method"},{"location":"ref/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"ref/#ESM_PINO.AbstractSphericalConv","page":"Reference","title":"ESM_PINO.AbstractSphericalConv","text":"AbstractSphericalConv <: Lux.AbstractLuxLayer\n\nAbstract supertype for spherical convolution layers.\n\nConcrete implementations are provided by extensions:\n\nESM_PINOQG3Ext.SphericalConv: QG3-based transforms.\nESM_PINOSpeedyWeatherExt.SphericalConv: SpeedyWeather transforms.\n\nLoad the corresponding extension to use a specific implementation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.BurgersFD","page":"Reference","title":"ESM_PINO.BurgersFD","text":"BurgersFD{T}\n\nFinite-difference matrices for discretising the 1D Burgers equation using periodic boundary conditions.\n\nInitialization\n\nBurgersFD(T, n, Δx=1)   BurgersFD(grid::Grid{T})\n\nArguments\n\nT::DataType: Element type\nn::Integer: Number of grid points\nΔx::Number: Grid spacing\ngrid::Grid{T}: Grid object\n\nFields\n\nM::AbstractMatrix{T}: Second-derivative (Laplacian) matrix with periodic BCs\nM2::AbstractMatrix{T}: First-derivative (central differences) matrix with periodic BCs\n\nDetails\n\nM approximates ∂²/∂x² with periodic wrap-around.\nM2 approximates ∂/∂x using central differences and periodic wrap-around.\nUse these matrices for semi-discrete formulations of Burgers’ equation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.BurgersFD2","page":"Reference","title":"ESM_PINO.BurgersFD2","text":"BurgersFD2{T}\n\nFinite-difference matrices for the 1D Burgers equation with periodic boundary conditions using a backward-difference discretisation for the convective (first-derivative) term.\n\nInitialization\n\nBurgersFD2(T, n, Δx=1)   BurgersFD2(grid::Grid{T})\n\nArguments\n\nT::DataType: Element type\nn::Integer: Number of grid points\nΔx::Number: Grid spacing\ngrid::Grid{T}: Grid object\n\nFields\n\nM::AbstractMatrix{T}: Second-derivative (Laplacian) matrix with periodic BCs\nM2::AbstractMatrix{T}: First-derivative matrix using backward differences (periodic BCs)\n\nDetails\n\nM is identical in form to BurgersFD's Laplacian (periodic).\nM2 is a backward-difference approximation of ∂/∂x; can improve stability for convection-dominated flows.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.BurgersFD_Dirichlet","page":"Reference","title":"ESM_PINO.BurgersFD_Dirichlet","text":"BurgersFD_Dirichlet{T}\n\nFinite-difference matrices for the 1D Burgers equation with Dirichlet boundary conditions.\n\nInitialization\n\nBurgersFD_Dirichlet(T, n, Δx=1)   BurgersFD_Dirichlet(grid::Grid{T})\n\nArguments\n\nT::DataType: Element type\nn::Integer: Number of grid points\nΔx::Number: Grid spacing\ngrid::Grid{T}: Grid object\n\nFields\n\nM::AbstractMatrix{T}: Second-derivative matrix with Dirichlet enforcement at boundaries\nM2::AbstractMatrix{T}: First-derivative matrix with boundary rows/cols zeroed\n\nDetails\n\nBoundary rows/columns are zeroed to reflect fixed-value (Dirichlet) conditions.\nIntended for Burgers’ problems with fixed boundary values (e.g., u=0 at domain ends).\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.ChannelMLP","page":"Reference","title":"ESM_PINO.ChannelMLP","text":"ChannelMLP(channels::Int; expansion_factor=2.0, activation=gelu)\n\nImplements a channel-wise MLP with a skip connection.   Expects input in (height, width, channels, batch) format.\n\nArguments\n\nchannels: Number of input/output channels\nexpansion_factor: Factor to expand hidden layer size (default: 2.0)\nactivation: Nonlinear activation function in hidden layer (default: NNlib.gelu)\n\nFields\n\nmlp::M: Two-layer Conv-based MLP with hidden dimension = expansion_factor * channels\nskip::S: Skip connection implemented as a SoftGating layer\nexpansion_factor::Number: Factor controlling hidden dimension size\n\nDetails\n\nExpands channels with a 1x1 convolution, applies nonlinearity, then projects back\nAdds gated skip connection to stabilize training\nFunctions similarly to a feed-forward block in transformers\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.ChannelMLP1D","page":"Reference","title":"ESM_PINO.ChannelMLP1D","text":"ChannelMLP1D(channels::Int; expansion_factor=0.5, activation=gelu)\n\nImplements a channel-wise MLP with a skip connection.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.ChannelMLP3D","page":"Reference","title":"ESM_PINO.ChannelMLP3D","text":"ChannelMLP3D(channels::Int; expansion_factor=0.5, activation=gelu)\n\nImplements a channel-wise MLP with a skip connection.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.FDPhysicsLossParameters","page":"Reference","title":"ESM_PINO.FDPhysicsLossParameters","text":"FDPhysicsLossParameters(ν::Float64, N_t::Int, t_max::Float64, t_min::Float64, Δt::Float64, x_σ::Float64, x_μ::Float64, M1_gpu::AbstractArray, M2_gpu::AbstractArray)\n\nCreate a struct to hold parameters for finite difference physics loss.\n\nFields\n\nν: Viscosity (scalar)\n'tsteplength`: Time step length (scalar)\nM1_gpu: Second derivative FD matrix (GPU array)\nM2_gpu: First derivative FD matrix (GPU array)\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.FNO_Block","page":"Reference","title":"ESM_PINO.FNO_Block","text":"A block that combines a SpectralKernel with optional normalization and a ChannelMLP.   Expects input in (height, width, channels, batch) format.\n\nArguments\n\nchannels: Number of input/output channels\nmodes: Tuple specifying number of low-frequency modes for the spectral convolution\nexpansion_factor: Factor controlling hidden dimension size in ChannelMLP (default: 2)\nactivation: Nonlinear activation function (default: NNlib.gelu)\nuse_norm: Whether to use instance normalization after spectral kernel (default: false)\n\nFields\n\nspectral_kernel::SpectralKernel: Combines spectral and spatial convolutions\nnorm::Union{Lux.InstanceNorm, Lux.NoOpLayer}: Optional normalization layer\nchannel_mlp::ChannelMLP: Channel-wise MLP with skip connection\nchannels::Int: Number of channels\nmodes::NTuple{2, Int}: Retained Fourier modes\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.FNO_Block1D","page":"Reference","title":"ESM_PINO.FNO_Block1D","text":"FNO_Block1D\n\nA block that combines a spectral kernel with a channel MLP. \n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.FNO_Block3D","page":"Reference","title":"ESM_PINO.FNO_Block3D","text":"FNO_Block3D\n\nA block that combines a spectral kernel with a channel MLP. \n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.FourierNeuralOperator","page":"Reference","title":"ESM_PINO.FourierNeuralOperator","text":"A Fourier Neural Operator (FNO) container that optionally includes positional embeddings, lifting and projection convolutions, and a stack of FNO blocks.\n\nArguments\n\nin_channels::Int: Number of input channels.\nout_channels::Int: Number of output channels.\nhidden_channels::Int=32: Number of hidden channels used inside FNO blocks.\nn_modes::NTuple{N,Int}=(16, 16): Number of retained Fourier modes per spatial dimension.\nn_layers::Int=4: Number of FNO blocks to stack.\nlifting_channel_ratio::Int=2: Channel expansion ratio used in the lifting layer.\nprojection_channel_ratio::Int=2: Channel expansion ratio used in the projection layer.\nchannel_mlp_expansion::Number=2: Expansion factor inside ChannelMLP of each block.\nactivation=NNlib.gelu: Activation function used in conv layers.\npositional_embedding::AbstractString=\"grid\": Choice of positional embedding:\n\n\"grid\", \"nogrid\" => 2D variants (GridEmbedding2D or NoOpLayer) \"grid1D\", \"nogrid1D\" => 1D variants (GridEmbedding1D or NoOpLayer) \"grid3D\", \"no_grid3D\" => 3D variants (GridEmbedding3D or NoOpLayer)\n\nuse_norm_in_blocks::Bool=false: Whether to use normalization layers inside FNO blocks.\n\nFields\n\nembedding: Positional embedding layer (a GridEmbeddingND or NoOpLayer).\nlifting: Lifting convolution(s) mapping inchannels -> hiddenchannels.\nfno_blocks: Repeated stack of FNO blocks appropriate to dimensionality.\nprojection: Projection convolution(s) mapping hiddenchannels -> outchannels.\n\nExamples\n\nExample (2D data with grid embedding):\n\nusing Lux, Random, ESM_PINO\n\nrng = Random.default_rng()\n\nlayer = FourierNeuralOperator(\n    in_channels=3,\n    out_channels=2,\n    hidden_channels=32,\n    n_modes=(12, 12),\n    n_layers=4,\n    positional_embedding=\"grid\"\n)\n\nps = Lux.initialparameters(rng, layer)\nst = Lux.initialstates(rng, layer)\n\n# Input tensor (H, W, C, Batch)\nx = randn(Float32, 64, 64, 3, 10)\n\ny, st_new = layer(x, ps, st)\n@show size(y)   # expect (64, 64, 2, 10)\n\nAnother FNO example (1D data without grid embedding):\n\nusing Lux, Random, ESM_PINO\n\nlayer1d = FourierNeuralOperator(\n    in_channels=1,\n    out_channels=1,\n    hidden_channels=16,\n    n_modes=(8,),\n    n_layers=3,\n    positional_embedding=\"no_grid1D\"\n)\n\nx1 = randn(Float32, 128, 1, 5)   # (L, C, Batch)\ny1, _ = layer1d(x1,\n    Lux.initialparameters(rng, layer1d),\n    Lux.initialstates(rng, layer1d)\n)\n@show size(y1)   # expect (128, 1, 5)\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.Grid","page":"Reference","title":"ESM_PINO.Grid","text":"Grid{T}\n\nDiscretization grid for 1D finite difference schemes.\n\nInitialization\n\nGrid(x)\n\nwith x an array or range with constant spacing.\n\nArguments\n\nx::AbstractVector{T}: Discretization points, assumed uniformly spaced.\n\nFields\n\nN::Int: Number of grid points\nx::AbstractVector{T}: Coordinates of grid points\nΔx::T: Grid spacing, computed from x\n\nDetails\n\nComputes Δx as the absolute difference between the first two grid points.\nUseful for constructing finite-difference scheme matrices.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.GridEmbedding2D","page":"Reference","title":"ESM_PINO.GridEmbedding2D","text":"GridEmbedding2D(grid_boundaries=[[0f0, 1f0], [0f0, 1f0]])\n\nPositional embedding that appends normalized 2D coordinates to the input.   Expects input in (height, width, channels, batch) format.\n\nArguments\n\ngrid_boundaries: Vector of two intervals [x_min, x_max], [y_min, y_max] specifying coordinate range along each axis\n\nFields\n\nboundaries_x::Vector{Float32}: Range boundaries for x-coordinate\nboundaries_y::Vector{Float32}: Range boundaries for y-coordinate\n\nDetails\n\nConstructs a 2D meshgrid of coordinates normalized to [x_min, x_max] × [y_min, y_max]\nRepeats coordinate grids across batch dimension\nConcatenates grid_x and grid_y as extra channels to the input\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.GridEmbedding3D","page":"Reference","title":"ESM_PINO.GridEmbedding3D","text":"GridEmbedding3D(grid_boundaries=[[0f0, 1f0], [0f0, 1f0], [0f0, 1f0]])\n\nPositional embedding that appends a normalized 3D coordinate grid to input data.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.NeumannFD","page":"Reference","title":"ESM_PINO.NeumannFD","text":"NeumannFD{T}\n\nFinite-difference operator for the first derivative with Neumann boundary conditions (enforcing zero derivative at the boundaries).\n\nInitialization\n\nNeumannFD(T, n, Δx=1)   NeumannFD(grid::Grid{T})\n\nArguments\n\nT::DataType: Element type (e.g. Float64, Float32)\nn::Integer: Number of grid points\nΔx::Number: Grid spacing (default: 1)\ngrid::Grid{T}: Grid object containing N and Δx\n\nFields\n\nM::AbstractMatrix{T}: Finite-difference matrix representing the derivative operator\n\nDetails\n\nImplements central differences for the interior and modifies boundary rows to enforce zero slope.\nThe returned operator approximates ∂/∂x with Neumann BCs.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.PeriodicFD","page":"Reference","title":"ESM_PINO.PeriodicFD","text":"PeriodicFD{T}\n\nFinite-difference operator for the first derivative with periodic boundary conditions.\n\nInitialization\n\nPeriodicFD(T, n, Δx=1)   PeriodicFD(grid::Grid{T})\n\nArguments\n\nT::DataType: Element type\nn::Integer: Number of grid points\nΔx::Number: Grid spacing (default: 1)\ngrid::Grid{T}: Grid object\n\nFields\n\nM::AbstractMatrix{T}: Finite-difference matrix representing the derivative operator\n\nDetails\n\nImplements central differences and wraps the stencil at the domain boundaries (periodic).\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SFNO","page":"Reference","title":"ESM_PINO.SFNO","text":"Empty layer to test extension documentation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SFNO_Block","page":"Reference","title":"ESM_PINO.SFNO_Block","text":"Empty layer to test extension documentation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SoftGating","page":"Reference","title":"ESM_PINO.SoftGating","text":"SoftGating(channels::Int)\n\nA soft gating layer that applies per-channel multiplicative scaling.   Expects input in (height, width, channels, batch) format.\n\nArguments\n\nchannels: Number of channels in the input\n\nFields\n\nchannels::Int: Number of channels\n\nDetails\n\nLearns a single scalar weight per channel\nWeights are initialized to 1.0 (identity scaling)\nUseful for lightweight residual or skip connections\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SoftGating1D","page":"Reference","title":"ESM_PINO.SoftGating1D","text":"SoftGating1D(channels::Int)\n\nA soft gating layer that applies per-channel multiplicative scaling.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SoftGating3D","page":"Reference","title":"ESM_PINO.SoftGating3D","text":"SoftGating3D(channels::Int)\n\nA soft gating layer that applies per-channel multiplicative scaling.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SpectralConv","page":"Reference","title":"ESM_PINO.SpectralConv","text":"SpectralConv{T,N}\n\nSpectral convolution layer for Fourier Neural Operator in Lux.jl. Expects input in (spatial..., channel, batch) format.\n\nArguments\n\nin_channels: Number of input channels\nout_channels: Number of output channels\nmodes: Tuple specifying number of low-frequency modes to retain along each spatial dimension\nT: Data type for weights (default: ComplexF32)\nN: Number of spatial dimensions (inferred from length of modes)\n\nFields\n\nin_channels::Int: Number of input channels\nout_channels::Int: Number of output channels\nmodes::NTuple{N,Int}: Number of low-frequency modes to retain along each spatial dimension\n\nDetails\n\nUses FFT to transform input to frequency domain, applies learned complex weights to low-frequency modes, and transforms back to spatial domain\nPads output back to original spatial dimensions after truncation\nWeights are initialized with Glorot-like scaling\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SpectralKernel","page":"Reference","title":"ESM_PINO.SpectralKernel","text":"SpectralKernel{P,F}\n\nCombines a SpectralConv layer with a 1x1 convolution in parallel, followed by an activation function.   Expects input in (spatial..., channel, batch) format.\n\nArguments\n\nin_ch: Number of input channels\nout_ch: Number of output channels\nmodes: Tuple specifying number of low-frequency modes to retain in the spectral branch\nactivation: Activation function applied after combining spatial and spectral branches (default: NNlib.gelu)\n\nFields\n\nspatial_conv::P: 1x1 convolution operating directly in the spatial domain\nspectral_conv::SpectralConv: Spectral convolution layer\nactivation::F: Elementwise activation function\n\nDetails\n\nThe input is processed in parallel by a 1x1 convolution and a spectral convolution\nOutputs from both branches are summed and passed through the activation\nUseful for mixing local (spatial) and global (spectral) information\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SpectralKernel1D","page":"Reference","title":"ESM_PINO.SpectralKernel1D","text":"SpectralKernel1D{P,F}\nCombines a SpectralConv layer with a 1x1 convolution in parallel, followed by an activation function.\n\nExpects input in (spatial, channel, batch) format.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SpectralKernel3D","page":"Reference","title":"ESM_PINO.SpectralKernel3D","text":"SpectralKernel3D{P,F}\n\nCombines a SpectralConv layer with a 1x1 convolution in parallel, followed by an activation function. Expects input in (spatial..., channel, batch) format.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SpectralPhysicsLossParameters","page":"Reference","title":"ESM_PINO.SpectralPhysicsLossParameters","text":"SpectralPhysicsLossParameters(ν::Float64, L::Float64, N_t::Int, t_max::Float64, t_min::Float64, Δt::Float64, x_σ::Float64, x_μ::Float64)\n\nCreate a struct to hold parameters for spectral physics loss.\n\nFields\n\nν: Viscosity (scalar)\nL: Domain size (scalar)\nN_t: Number of time steps (integer)\nt_max: Maximum time (scalar)\nt_min: Minimum time (scalar)\nΔt: Time step size (scalar)\nx_σ: Standard deviation for normalization (scalar)\nx_μ: Mean for normalization (scalar)\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SphericalConv","page":"Reference","title":"ESM_PINO.SphericalConv","text":"Empty layer to test extension documentation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.SphericalKernel","page":"Reference","title":"ESM_PINO.SphericalKernel","text":"Empty layer to test extension documentation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#ESM_PINO.add_noise-Tuple{AbstractArray}","page":"Reference","title":"ESM_PINO.add_noise","text":"add_noise(data::AbstractArray; noise_level::Real=0.1, noise_type::Symbol=:gaussian, relative::Bool=true, rng::AbstractRNG=Random.GLOBAL_RNG)\n\nAdd random noise to an array, supporting Gaussian or uniform distributions.\n\nArguments\n\ndata::AbstractArray: Input array to which noise will be added.\nnoise_level::Real=0.1: Magnitude of the noise. Interpreted as standard deviation for Gaussian or half-width for uniform.\nnoise_type::Symbol=:gaussian: Type of noise distribution. Options: :gaussian or :uniform.\nrelative::Bool=true: If true, scale the noise level relative to the standard deviation of data.\nrng::AbstractRNG=Random.GLOBAL_RNG: Random number generator.\n\nReturns\n\nArray: A copy of data with added noise.\n\nDetails\n\nFor :gaussian noise, samples are drawn from a normal distribution with mean 0 and specified standard deviation.\nFor :uniform noise, samples are drawn uniformly from [-noise_level, noise_level].\nIf relative=true, the noise magnitude is scaled by the standard deviation of data.\n\nExample\n\njulia> x = rand(10);\n\njulia> y = add_noise(x; noise_level=0.05, noise_type=:gaussian);\n\njulia> z = add_noise(x; noise_level=0.2, noise_type=:uniform, relative=false);\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.apply_pattern-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, AbstractArray{T}}} where {T, N}","page":"Reference","title":"ESM_PINO.apply_pattern","text":"apply_pattern(x_tr::AbstractArray{T,N}, weights::AbstractArray{T,3}) where {T,N}\n\nApply learned weight patterns to truncated Fourier coefficients.\n\nArguments\n\nx_tr::AbstractArray{T,N}: Truncated Fourier coefficients after low-pass filtering, with shape (modes..., in_channels, batch)\nweights::AbstractArray{T,4}: Complex-valued learned weights with shape (modes..., outchannels, inchannels)\n\nReturns\n\nWeighted Fourier coefficients with shape (modes..., out_channels, batch)\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.autoregressive_loss-Tuple{LuxCore.StatefulLuxLayerImpl.StatefulLuxLayer, Tuple{AbstractArray, AbstractArray}, Int64, ESM_PINO.FDPhysicsLossParameters, Float32}","page":"Reference","title":"ESM_PINO.autoregressive_loss","text":"autoregressive_loss(model::StatefulLuxLayer, (u0, target)::Tuple{AbstractArray, AbstractArray}, n_steps::Int, params::FDPhysicsLossParameters, λ::Float32)\n\nCompute autoregressive loss for a model over multiple time steps.\n\nArguments\n\nmodel: StatefulLuxLayer model\nu0: Initial state (input data)\ntarget: Target data for comparison\nn_steps: Number of time steps to propagate\nparams: FDPhysicsLossParameters struct containing physics parameters\nλ: Weighting factor for physics loss\n\nReturns\n\nTotal loss combining data loss and physics-informed loss\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.compute_gaussian_latitudes-Tuple{Int64}","page":"Reference","title":"ESM_PINO.compute_gaussian_latitudes","text":"Compute Gaussian latitudes using Newton's method to find roots of Legendre polynomials.\nReturns latitudes in radians, sorted from North to South (decreasing order).\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.compute_k-Union{Tuple{T}, Tuple{AbstractArray{T}, T}} where T<:Real","page":"Reference","title":"ESM_PINO.compute_k","text":"compute_k(u::AbstractArray{T}, L::T) where T<:Real\n\nGenerate wavenumber array for spectral differentiation.\n\nArguments\n\nu: Template array for dimensions\nL: Domain length\n\nReturns\n\nk: Wavenumber array on GPU, reshaped for broadcasting\n\nDetails\n\nHandles even/odd array sizes differently\nAutomatically converts to GPU array\nReturns array with singleton dimensions for ND broadcasting\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.create_physics_loss-Tuple{ESM_PINO.SpectralPhysicsLossParameters}","page":"Reference","title":"ESM_PINO.create_physics_loss","text":"create_physics_loss()\n\nhelper function to create a physics loss function.\n\nArguments\n\nparams: parameters struct, pass nothing to create a zero loss function.\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.dealias-Union{Tuple{T}, Tuple{AbstractArray{Complex{T}}, T}} where T<:Real","page":"Reference","title":"ESM_PINO.dealias","text":"dealias(u_hat::AbstractArray{Complex{T}}, L::T) where T<:Real\n\nApply 2/3 dealiasing filter to Fourier coefficients.\n\nArguments\n\nu_hat: Fourier coefficients (complex array)\nL: Domain length (unused in current implementation)\n\nReturns\n\nFiltered coefficients with high frequencies zeroed\n\nNotes\n\nImplements 2/3 rule for anti-aliasing\nCreates mask directly on GPU\nPreserves array dimensions for broadcasting\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.expand_pad_dims-Union{Tuple{NTuple{N, Int64}}, Tuple{N}} where N","page":"Reference","title":"ESM_PINO.expand_pad_dims","text":"expand_pad_dims(pad_dims::Dims{N}) where {N}\n\nConvert N-dimensional padding specification into format required for NNlib's pad_constant function.\n\nArguments\n\npad_dims::Dims{N}: Tuple of N integers specifying the total padding needed along each dimension\n\nReturns\n\nNTuple{2N,Int}: Tuple of 2N integers specifying padding for both sides of each dimension, where padding is applied only at the end of each dimension (start padding is always 0)\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.gaussian_grid-Tuple{Int64}","page":"Reference","title":"ESM_PINO.gaussian_grid","text":"Generate Gaussian grid with proper Gaussian latitudes using Legendre polynomials.\nReturns latitudes in radians, sorted from North to South (decreasing order).\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.legendre_polynomial-Tuple{Int64, Float64}","page":"Reference","title":"ESM_PINO.legendre_polynomial","text":"Compute Legendre polynomial P_n(x) and its derivative using recurrence relation.\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.low_pass-Tuple{AbstractArray, Any}","page":"Reference","title":"ESM_PINO.low_pass","text":"low_pass(x_ft, modes)\n\nApply a low-pass filter to a Fourier-transformed array by retaining only the lowest frequency modes.\n\nArguments\n\nx_ft: A Fourier-transformed array with at least 2 trailing dimensions\nmodes: A tuple or array specifying the number of low-frequency modes to keep along each leading dimension\n\nReturns\n\nA view of the input array x_ft containing only the specified low-frequency modes, preserving the last two dimensions in full\n\nDetails\n\nThe function creates a view that selects the first modes[i] elements along each leading dimension i, while keeping all elements of the last two dimensions. This effectively implements a low-pass filter in Fourier space by truncating high-frequency modes.\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.meshgrid-Tuple{Any, Any}","page":"Reference","title":"ESM_PINO.meshgrid","text":"meshgrid(x, y)\n\nGenerates a 2D meshgrid from vectors x and y.\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.mse_loss_function-Tuple{LuxCore.StatefulLuxLayerImpl.StatefulLuxLayer, AbstractArray, AbstractArray}","page":"Reference","title":"ESM_PINO.mse_loss_function","text":"mse_loss_function(u::StatefulLuxLayer, target::AbstractArray, xt::AbstractArray)\n\nStandard mean squared error loss.\n\nArguments\n\nu: Neural network\ntarget: Ground truth values\nu_t1: Network inputs\n\nReturns\n\nMSE between network output and target\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.normalize_data-Tuple{Any}","page":"Reference","title":"ESM_PINO.normalize_data","text":"normalize_data(data)\n\nNormalize an array to zero mean and unit variance.\n\nArguments\n\n-data: Input array.\n\nReturns\n\n-(normalized_data, μ, σ): A tuple containing:     normalized_data: The normalized array.     μ: The mean of the original data.     σ: The standard deviation of the original data.\n\n\n\n\n\n","category":"method"},{"location":"ref/#ESM_PINO.select_loss_function","page":"Reference","title":"ESM_PINO.select_loss_function","text":"select_loss_function()\n\nHelper function to pass a valid loss function to Training.singletrainstep. Selects a loss function based on the provided physics-informed loss function, in the standard workflow generated with createphysicsloss.\n\nArguments\n\nPI_loss: Physics-informed loss function (default is a zero loss function)\n\n\n\n\n\n","category":"function"},{"location":"ref/#ESM_PINO.spectral_derivative-Union{Tuple{T}, Tuple{AbstractArray{T}, T}} where T<:Real","page":"Reference","title":"ESM_PINO.spectral_derivative","text":"spectral_derivative(u::AbstractArray{T}, L::T) where T<:Real\n\nCompute first and second spatial derivatives using FFT spectral methods.\n\nArguments\n\nu: Input array (real-valued), assumed to be on GPU. First dimension is spatial.\nL: Domain length in spatial dimension.\n\nReturns\n\ndu: First derivative (real array)\nd2u: Second derivative (real array)\n\nNotes\n\nUses FFT/iFFT with wavenumbers from compute_k\nAssumes periodic boundary conditions\nMaintains input array type/location (GPU/CPU)\nOutput derivatives are real-valued arrays\n\n\n\n\n\n","category":"method"},{"location":"#ESM_PINO","page":"Home","title":"ESM_PINO","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ESM_PINO.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The SFNO components are empty-defined in the main module and work through extensions by loading two different backends for the Spherical Harmonics transforms: QG3.jl (supports Zygote for automatic differentiation), and SpeedyWeather.jl (still work in progress)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"ref.md\", \"extensions/QG3.md\", \"extensions/SpeedyWeather.md\"]\nDepth = 2","category":"page"}]
}
